{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning DistilBERT for Offensive Content Detection\n",
    "\n",
    "This notebook fine-tunes a DistilBERT model for classifying Discord messages as Hate Speech (0), Offensive (1), or Neither (2). It uses the same datasets and preprocessing as `lstm_finetune_clean.ipynb` but applies transfer learning with DistilBERT. The model is converted to TFLite for real-time inference in a Discord bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.compat.v1.train import AdamOptimizer\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import emoji\n",
    "import hashlib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix: Explicitly import callbacks from tf.keras (not standalone keras)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi\n",
    "max_length = 30\n",
    "batch_size = 16\n",
    "\n",
    "# Slang normalization (sama dengan lstm_finetune_clean.ipynb)\n",
    "slang_dict = {\n",
    "    'wtf': 'what the fuck', 'lol': 'laughing out loud', 'fr': 'for real', 'tbh': 'to be honest',\n",
    "    'fucking': 'fuckin', 'fuckinng': 'fuckin', 'ur': 'your', 'r': 'are',\n",
    "    'omg': 'oh my god', 'dope': 'great', 'lit': 'great', 'nigga': 'nigga',\n",
    "    'pussi': 'pussy', 'hoe': 'ho', 'fam': 'friends', 'dawg': 'friend',\n",
    "    'stfu': 'shut up', 'yo': 'hey', 'vibin': 'vibing', 'chill': 'relax',\n",
    "    'slaps': 'great', 'cap': 'lie', 'bet': 'okay'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?]', '', text)\n",
    "    for slang, full in slang_dict.items():\n",
    "        text = re.sub(r'\\b' + slang + r'\\b', full, text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def hash_text(text):\n",
    "    return hashlib.md5(str(text).encode()).hexdigest().lower()\n",
    "\n",
    "def load_and_clean(df, text_col, label_col=None, rename=True):\n",
    "    if rename and label_col:\n",
    "        df = df.rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    elif rename:\n",
    "        df = df.rename(columns={text_col: 'text'})\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    df['hash'] = df['text'].apply(hash_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df = pd.read_csv('data/labeled_data_clean.csv')\n",
    "df = load_and_clean(df, 'clean_tweet', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = pd.read_csv('data/toxic-chat_annotation_all.csv')\n",
    "toxic_data['label'] = toxic_data['toxicity'].apply(lambda x: 1 if x == 1 else 2)\n",
    "toxic_data = load_and_clean(toxic_data, 'user_input', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADVAN\\AppData\\Local\\Temp\\ipykernel_3844\\3462819524.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "oig_data = pd.read_csv('data/OIG_safety_v0.2.csv')\n",
    "offensive_keywords = ['abuse', 'hate', 'offensive', 'bastard', 'fuck', 'bitch', 'nigga', 'asshole']\n",
    "oig_data['label'] = oig_data['text'].apply(\n",
    "    lambda x: 1 if any(word in str(x).lower() for word in offensive_keywords) else 2\n",
    ")\n",
    "oig_data = load_and_clean(oig_data, 'text', 'label', rename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_data = pd.read_csv('data/Example-Data-Guild_966767749118443530.csv')\n",
    "discord_data['label'] = discord_data['content'].apply(\n",
    "    lambda x: 1 if any(word in str(x).lower() for word in ['bastard', 'fuck', 'bitch']) else 2\n",
    ")\n",
    "discord_data = load_and_clean(discord_data, 'content', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_data = pd.DataFrame({\n",
    "    'text': ['yo dawg this is lit sunglasses', 'omg so cool fr', 'fam we vibin', 'stfu you jerk', 'bastard you suck',\n",
    "             'this game slaps fr', 'chill vibes only', 'wtf this is dope', 'cap you lyin fam'],\n",
    "    'label': [2, 2, 2, 1, 1, 2, 2, 2, 1]\n",
    "})\n",
    "real_world_data['text'] = real_world_data['text'].apply(clean_text)\n",
    "real_world_data = real_world_data[real_world_data['text'].str.len() > 0]\n",
    "real_world_data['hash'] = real_world_data['text'].apply(hash_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "df = pd.concat([df[['text', 'label', 'hash']], toxic_data[['text', 'label', 'hash']],\n",
    "                oig_data[['text', 'label', 'hash']], discord_data[['text', 'label', 'hash']],\n",
    "                real_world_data[['text', 'label', 'hash']]], ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['hash']).drop(columns=['hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "df_hate = df[df.label == 0]\n",
    "df_offensive = df[df.label == 1]\n",
    "df_neither = df[df.label == 2]\n",
    "df_hate_upsampled = resample(df_hate, replace=True, n_samples=25000, random_state=42)\n",
    "df_offensive_upsampled = resample(df_offensive, replace=True, n_samples=25000, random_state=42)\n",
    "df_neither_upsampled = resample(df_neither, replace=True, n_samples=25000, random_state=42)\n",
    "df_balanced = pd.concat([df_hate_upsampled, df_offensive_upsampled, df_neither_upsampled])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented data\n",
    "augmented_df = pd.read_csv('data/augmented_data.csv')\n",
    "df_balanced = pd.concat([df_balanced, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for BERT (clean and convert to strings)\n",
    "texts = [str(text).strip() if text is not None else \"\" for text in df_balanced['text']]\n",
    "labels = df_balanced['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['phone drier hey bitch scalp', 'peac fag rememb im best lux support na drop lustboy pick kakao tsm', 'lesbian get nip pierc ew']\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))\n",
    "print(texts[:3])  # preview the first few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    encodings['input_ids'].numpy(), to_categorical(labels, num_classes=3),\n",
    "    test_size=0.15, random_state=42, stratify=labels\n",
    ")\n",
    "attention_mask_train, attention_mask_test = train_test_split(\n",
    "    encodings['attention_mask'].numpy(), test_size=0.15, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': X_train, 'attention_mask': attention_mask_train},\n",
    "    y_train\n",
    ")).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': X_test, 'attention_mask': attention_mask_test},\n",
    "    y_test\n",
    ")).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ADVAN\\machine-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2356/4516 [==============>...............] - ETA: 18:07 - loss: 10.0498 - accuracy: 0.3144"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Compile with TF-compatible optimizer\n",
    "optimizer = AdamOptimizer(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])  # or sparse_categorical_crossentropy\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint('model/lstm/bert_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_pred = model.predict(test_dataset)\n",
    "test_pred_classes = np.argmax(test_pred.logits, axis=1)\n",
    "test_true_classes = np.argmax(y_test, axis=1)\n",
    "print(\"\\nClassification Report (Test - BERT):\")\n",
    "print(classification_report(test_true_classes, test_pred_classes, target_names=['Hate', 'Offensive', 'Neither']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true_classes, test_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Hate', 'Offensive', 'Neither'])\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - BERT (Test Set)')\n",
    "plt.show()\n",
    "\n",
    "# Training History\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained('model/bert/bert_model')\n",
    "tokenizer.save_pretrained('model/bert/bert_tokenizer')\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.experimental_new_converter = True\n",
    "converter.input_shape = (None, max_length)  # Batch size dinamis\n",
    "tflite_model = converter.convert()\n",
    "with open('model/lstm/tflite_bert_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "inference_tests = [\n",
    "    ('omg beautiful', 2), ('amazing', 2), ('holy', 2), ('omg', 2), ('bastard', 1),\n",
    "    ('hello guys im here', 2), ('great bro', 2), ('nigger', 0), ('youre a pathetic loser', 1),\n",
    "    ('lets go', 2), ('are you serious', 2), ('wtf', 2), ('lets fuckin go', 2),\n",
    "    ('omg how this grok so idiot', 1), ('not bad', 2), ('well done', 2),\n",
    "    ('had a long day need a nap', 2), ('this is fuckin awesome', 2),\n",
    "    ('yo this game is dope sunglasses', 2), ('what a bitch always complaining', 1),\n",
    "    ('this app is a ripoff pure garbage', 1), ('muslims are terrorists ban them all', 0),\n",
    "    ('black people are just lazy leeches', 0), ('beautiful weather for a walk today', 2),\n",
    "    ('i love you', 2), ('i mean thats alright', 2), ('just stop doing that', 2),\n",
    "    ('why are you doing that', 2), ('why you are so dumb', 1), ('dawg do you even had a brain', 1),\n",
    "    ('indian nigga', 0), ('indian jerk bastard', 1), ('stfu indian', 0),\n",
    "    ('fr this game slaps bet', 2), ('vibin with the fam yo', 2)\n",
    "]\n",
    "\n",
    "test_texts = [clean_text(text) for text, _ in inference_tests]\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}\n",
    ")).batch(1)\n",
    "\n",
    "test_preds = model.predict(test_dataset)\n",
    "test_pred_classes = np.argmax(test_preds.logits, axis=1)\n",
    "\n",
    "for (text, true_label), pred_class, probs, cleaned in zip(inference_tests, test_pred_classes, test_preds.logits, test_texts):\n",
    "    class_names = ['Hate', 'Offensive', 'Neither']\n",
    "    print(f'Input: {text} (Cleaned: {cleaned})')\n",
    "    print(f'True Class: {class_names[true_label]}')\n",
    "    print(f'Predicted Class: {class_names[pred_class]}, Confidence: {tf.nn.softmax(probs).numpy()[pred_class]:.4f}')\n",
    "    print(f'Scores: Hate={tf.nn.softmax(probs).numpy()[0]:.4f}, Offensive={tf.nn.softmax(probs).numpy()[1]:.4f}, Neither={tf.nn.softmax(probs).numpy()[2]:.4f}\\n')\n",
    "\n",
    "correct = sum(p == t for p, (_, t) in zip(test_pred_classes, inference_tests))\n",
    "accuracy = correct / len(inference_tests)\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dokumentasi\n",
    "print('Transfer Learning: Fine-tuned DistilBERT untuk klasifikasi Hate Speech/Offensive/Neither.')\n",
    "print('Dataset: OIG_safety_v0.2.csv, labeled_data_clean.csv, toxic-chat_annotation_all.csv, Discord Unveiled, real-world data.')\n",
    "print('Model disimpan di model/lstm/bert_model dan tflite_bert_model.tflite.')\n",
    "print('Memenuhi aturan: Utilize TensorFlow, transfer learning allowed, no TensorFlow Hub.')\n",
    "print('LSTM model retained as backup for comparison in inference.py.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
